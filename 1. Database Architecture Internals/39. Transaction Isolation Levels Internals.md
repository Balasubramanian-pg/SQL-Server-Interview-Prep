# 39. Transaction Isolation Levels Internals

Canonical documentation for [39. Transaction Isolation Levels Internals](1. Database Architecture Internals/39. Transaction Isolation Levels Internals.md). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of transaction isolation levels is to define the degree to which the operations within one transaction are visible to, and affected by, concurrent operations in other transactions. In a multi-user database environment, concurrent access to shared data is necessary for performance, but it introduces the risk of data inconsistencies. Isolation levels provide a standardized framework for balancing the trade-off between data consistency and system throughput.

By defining specific isolation levels, systems allow developers to choose the appropriate "strictness" for a given workload, ensuring that business logic remains correct even when multiple processes manipulate the same records simultaneously.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the theoretical underpinnings defined by the ANSI/ISO SQL standard and extended by modern concurrency control research.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * The four standard ANSI/ISO isolation levels.
> * Read phenomena (anomalies) that isolation levels are designed to prevent.
> * Theoretical mechanisms of isolation, including locking and Multi-Version Concurrency Control (MVCC).
> * Consistency guarantees and performance trade-offs.

> [!WARNING]
> **Out of scope:**
> * Specific vendor-specific syntax or configuration flags (e.g., specific PostgreSQL, MySQL, or Oracle commands).
> * Distributed transaction protocols (e.g., Two-Phase Commit) except where they intersect with isolation theory.
> * Hardware-level atomic operations.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **Transaction** | A logical unit of work that must be treated as a single, indivisible operation (Atomic, Consistent, Isolated, Durable). |
| **Phenomenon** | An undesirable data state or observation resulting from concurrent transaction execution (e.g., a "Dirty Read"). |
| **Concurrency Control** | The mechanism (locking, versioning, or validation) used by a system to enforce isolation. |
| **Strict Serializability** | The highest level of consistency where the outcome of concurrent transactions is equivalent to some sequential execution. |
| **Snapshot** | A consistent view of the data at a specific point in time, often used in MVCC implementations. |
| **Write Skew** | A phenomenon where two transactions read overlapping data sets, make disjoint updates, and violate a combined constraint. |

## Core Concepts
The fundamental challenge of transaction isolation is the **Concurrency vs. Consistency Spectrum**. As a system moves toward higher isolation, it increases data integrity but typically decreases performance due to increased contention, blocking, or the overhead of managing multiple data versions.

### The "I" in ACID
Isolation ensures that concurrent transactions do not see each other's partial or intermediate states. Without isolation, a transaction might base its logic on data that is currently being modified or that will eventually be rolled back.

### Read Phenomena (Anomalies)
To understand isolation levels, one must first understand the anomalies they prevent:
1.  **Dirty Read:** A transaction reads data written by a concurrent uncommitted transaction. If the other transaction rolls back, the first transaction has read data that "never existed."
2.  **Non-repeatable Read:** A transaction reads the same row twice and finds different data because a concurrent committed transaction modified it in between.
3.  **Phantom Read:** A transaction executes a query returning a set of rows. A concurrent committed transaction inserts or deletes rows that match the query criteria. If the first transaction repeats the query, it sees a different set of rows.

> [!TIP]
> Think of isolation levels as a "filter" for reality. At lower levels, the filter is porous, letting in "noise" from other transactions. At the highest level, the filter is opaque, making the transaction feel as though it is the only process running on the system.

## Standard Model
The standard model for isolation levels is defined by the ANSI/ISO SQL-92 standard. It categorizes levels based on which phenomena they permit.

| Isolation Level | Dirty Read | Non-repeatable Read | Phantom Read |
|-----------------|------------|---------------------|--------------|
| **Read Uncommitted** | Possible | Possible | Possible |
| **Read Committed** | Not Possible | Possible | Possible |
| **Repeatable Read** | Not Possible | Not Possible | Possible |
| **Serializable** | Not Possible | Not Possible | Not Possible |

### 1. Read Uncommitted
The lowest level of isolation. Transactions can see changes made by others before they are committed. This provides maximum performance but zero reliability for data integrity.

### 2. Read Committed
The most common default level. A transaction only sees data that was committed before the read operation began. However, if the same read is performed twice, the data may change if another transaction committed in the interim.

### 3. Repeatable Read
Ensures that if a transaction reads a row, it will see the same values for that row throughout its duration. This is typically achieved by holding locks on the rows read or by providing a consistent snapshot.

### 4. Serializable
The gold standard. It guarantees that the result of executing transactions concurrently is the same as if they were executed one after another. This level prevents all standard anomalies, including those not explicitly named in the 1992 standard (like Write Skew).

> [!IMPORTANT]
> Modern systems often implement **Snapshot Isolation (SI)**. While SI prevents many anomalies and is often marketed as "Repeatable Read" or even "Serializable," it may still permit **Write Skew**, which a true Serializable level must prevent.

## Common Patterns
### Pessimistic Concurrency Control (Locking)
The system assumes conflicts will happen. It uses shared (S) locks for reads and exclusive (X) locks for writes. Transactions are blocked if they attempt to access data locked by another.

### Optimistic Concurrency Control (OCC)
The system assumes conflicts are rare. Transactions proceed without locks but check for conflicts at the time of commit. If a conflict is detected, the transaction is aborted and must be retried.

### Multi-Version Concurrency Control (MVCC)
The system maintains multiple versions of data objects. Readers look at a consistent snapshot of the data from a point in time, while writers create new versions. This allows "readers to not block writers, and writers to not block readers."

## Anti-Patterns
### Over-Isolation
Using `SERIALIZABLE` for every transaction regardless of the use case. This leads to excessive deadlocks, high latency, and reduced system throughput.

### The "Read-Modify-Write" Race
Reading data at a low isolation level (like Read Committed), performing a calculation in application code, and then writing the result back. This is prone to lost updates because the data may have changed between the read and the write.

> [!CAUTION]
> Never assume that "Repeatable Read" prevents all concurrency issues. In many implementations, it does not prevent "Lost Updates" if the application logic relies on non-atomic increments or checks.

## Edge Cases
### Write Skew
A classic edge case where two transactions (T1 and T2) both read a set of data (e.g., "count of doctors on call"). Both see that the count is 2. Both decide they can go off-call because the minimum required is 1. Both commit. The final count is 0, violating the business rule. Most "Repeatable Read" implementations allow this; only "Serializable" prevents it.

### Read-Only Anomalies
In some MVCC implementations, a read-only transaction can see an inconsistent state of the database if it runs concurrently with multiple update transactions, even if those update transactions are individually serializable.

### Phantom Reads in Range Queries
A transaction might lock all existing rows in a range (e.g., `WHERE age > 20`), but unless the system implements "Gap Locking" or "Predicate Locking," a new row with `age = 25` could be inserted by a concurrent transaction, appearing as a "phantom" in subsequent reads.

## Related Topics
*   **ACID Properties:** The broader context of transaction management.
*   **Locking Mechanisms:** Detailed study of S/X locks, Intent locks, and Deadlock detection.
*   **Distributed Consistency Models:** Concepts like Linearizability and Eventual Consistency in distributed systems.
*   **Write-Ahead Logging (WAL):** How durability is maintained alongside isolation.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-11 | Initial AI-generated canonical documentation |