# 35. Memory Optimized Filegroups

Canonical documentation for 35. Memory Optimized Filegroups. This document defines concepts, terminology, and standard usage.

## Purpose
Memory Optimized Filegroups exist to provide a specialized persistence layer for data structures that primarily reside in system memory. While memory-optimized engines process data at in-memory speeds, the data must still adhere to ACID (Atomicity, Consistency, Isolation, Durability) properties. 

The purpose of this filegroup is to manage the streaming of memory-resident data to non-volatile storage using a sequential I/O pattern. This ensures that in the event of a system failure or restart, the state of memory-optimized tables can be reconstructed accurately and efficiently.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural necessity of bridging volatile memory with persistent storage.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * The architectural role of specialized containers for memory-resident data.
> * The mechanism of Checkpoint File Pairs (CFPs).
> * The transition of data from volatile RAM to persistent disk.
> * Storage requirements for durability in high-performance engines.

> [!WARNING]
> **Out of scope:**
> * Specific syntax for vendor-specific SQL dialects.
> * General-purpose database filegroup management (standard row-store).
> * Hardware-level NVDIMM or Persistent Memory (PMEM) configuration.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Checkpoint File Pair (CFP) | A logical grouping of a Data file and a Delta file used to persist memory-optimized data. |
| Data File | A persistent file containing inserted rows within a specific transaction range. |
| Delta File | A persistent file containing identifiers for rows in the corresponding Data file that have been marked as deleted. |
| Streaming Storage | A storage method that prioritizes sequential writes over random access to maximize throughput. |
| Garbage Collection (GC) | The process of reclaiming space from CFPs that no longer contain active data or are superseded by newer checkpoints. |
| Root File | A metadata file within the filegroup that tracks the inventory and state of all associated data and delta files. |

## Core Concepts
The fundamental idea behind Memory Optimized Filegroups is the decoupling of the data's "working format" (optimized for CPU/RAM) from its "persistence format" (optimized for sequential disk I/O).

### Sequential Persistence
Unlike traditional filegroups that use random-access pages (usually 8KB), memory-optimized filegroups use a log-structured approach. Data is streamed to disk in large, sequential blocks. This minimizes disk head movement and maximizes the bandwidth of the underlying storage media.

### The Data/Delta Pair Mechanism
To handle updates and deletes without requiring random I/O, the system uses a dual-file approach:
1.  **Data Files:** Store new rows. Once a data file is closed, it is immutable.
2.  **Delta Files:** Store "tombstones" for rows in the data file that are no longer valid.

> [!TIP]
> Think of a Data file as a book where you can only add pages at the end, and a Delta file as a separate "errata" sheet that tells you which pages in the book are no longer true. To know the current state, you read the book and subtract the errata.

## Standard Model
The standard model for memory-optimized storage involves a dedicated container (or set of containers) within a database specifically marked for memory-optimized data.

1.  **Initialization:** The filegroup is defined at the database level.
2.  **Checkpointing:** As transactions occur in memory, a background process periodically flushes the "closed" transaction ranges to the filegroup.
3.  **Merge Process:** As the ratio of deleted rows (in Delta files) to total rows (in Data files) increases, the system merges pairs to reclaim space and maintain recovery performance.
4.  **Recovery:** During system startup, the engine reads the filegroup sequentially to reload the data into RAM.

## Common Patterns
*   **Multi-Container Distribution:** Spreading the memory-optimized filegroup across multiple physical volumes to increase I/O parallelism.
*   **High-Bandwidth Placement:** Placing these filegroups on the fastest available storage (e.g., NVMe) to reduce the "Recovery Time Objective" (RTO).
*   **Archive-Only Persistence:** Using the filegroup solely for schema-only tables (where data is not persisted) is a valid but distinct configuration where the filegroup exists but remains largely empty.

## Anti-Patterns
*   **Co-location with High-Latency Storage:** Placing memory-optimized filegroups on slow, spinning disks (HDD) or high-latency network shares, which bottlenecks the checkpoint process.
*   **Manual File Manipulation:** Attempting to move, rename, or delete individual files within the memory-optimized container via the operating system.
*   **Over-provisioning Containers:** Creating an excessive number of containers on the same physical drive, which can lead to I/O contention rather than performance gains.

> [!CAUTION]
> Never place memory-optimized filegroups on compressed file systems or volumes with deduplication enabled, as this interferes with the streaming write patterns and can lead to significant performance degradation.

## Edge Cases
*   **Storage Full Scenarios:** If the memory-optimized filegroup runs out of space, the in-memory engine cannot perform checkpoints. This may lead to the transaction log growing indefinitely and eventually halting all database modifications.
*   **Massive Deletions:** A sudden burst of deletes can create very large Delta files. If the merge process cannot keep up, recovery times may spike because the engine must process a high volume of "tombstones" during startup.
*   **Incomplete Merges:** If a system crashes during a merge operation of two CFPs, the recovery process must be able to identify the "stale" files versus the "new" merged file using the Root File metadata.

## Related Topics
*   **In-Memory OLTP Engines:** The processing engines that utilize these filegroups.
*   **Transaction Logging:** The relationship between the write-ahead log (WAL) and the checkpoint files.
*   **Recovery Time Objective (RTO):** The impact of filegroup size and speed on database startup time.
*   **ACID Properties:** The theoretical framework requiring the existence of persistent filegroups.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-11 | Initial AI-generated canonical documentation |