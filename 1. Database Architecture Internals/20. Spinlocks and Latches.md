# 20. Spinlocks and Latches

Canonical documentation for [20. Spinlocks and Latches](1. Database Architecture Internals/20. Spinlocks and Latches.md). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of spinlocks and latches is to provide low-level synchronization mechanisms that ensure mutual exclusion and data integrity in concurrent systems. These primitives address the need for protecting shared memory resources where the overhead of traditional operating system-level blocking (sleeping) would be disproportionately high compared to the expected wait time.

Spinlocks and latches are fundamental to the construction of high-performance kernels, device drivers, and database engines. They allow multiple execution threads to coordinate access to critical sections by utilizing hardware-level atomic instructions.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural and algorithmic behavior of these primitives rather than specific language syntax.

## Scope
This documentation covers the theoretical underpinnings, hardware requirements, and behavioral characteristics of spinlocks and latches.

> [!IMPORTANT]
> **In scope:**
> * Atomic primitives and hardware requirements (CAS, TAS).
> * Memory consistency and visibility requirements.
> * Performance trade-offs between spinning and blocking.
> * Scalability considerations in multi-core architectures.

> [!WARNING]
> **Out of scope:**
> * Specific vendor API implementations (e.g., POSIX pthreads, Windows API).
> * High-level language-specific syntax (e.g., Java `synchronized`, C++ `std::mutex`).
> * Application-level business logic synchronization.

## Definitions
| Term | Definition |
|------|------------|
| **Spinlock** | A synchronization primitive where a thread "spins" in a loop, repeatedly checking if a lock is available, rather than yielding control to the scheduler. |
| **Latch** | A short-term, low-level synchronization object used to protect internal data structures (common in database systems). Unlike locks, latches do not typically support deadlock detection. |
| **Atomic Operation** | An operation that completes in a single step relative to other threads, ensuring no intermediate state is visible. |
| **Critical Section** | A segment of code that accesses shared resources and must not be executed by more than one thread at a time. |
| **Context Switch** | The process of storing the state of a process or thread so that it can be restored and resume execution at a later point. |
| **Busy-waiting** | A technique in which a process repeatedly checks a condition until it becomes true, consuming CPU cycles. |
| **Memory Barrier** | A type of instruction that causes a central processing unit (CPU) or compiler to enforce an ordering constraint on memory operations. |

## Core Concepts
The fundamental principle behind spinlocks and latches is the utilization of **Atomic Primitives**. Modern CPUs provide instructions such as *Compare-and-Swap (CAS)* or *Test-and-Set (TAS)* that allow a processor to read and modify a memory location as a single, indivisible unit.

### The Cost of Context Switching
In traditional "heavyweight" locks (mutexes), if a lock is held by another thread, the requesting thread is put to sleep. This involves a context switch, which requires saving registers, updating scheduler queues, and flushing TLB entries. 

> [!TIP]
> Think of a spinlock like a person waiting for a bathroom stall by standing at the door and repeatedly checking the handle. A blocking lock is like taking a number and going to sleep in the lobby until a buzzer wakes you up. If the person inside only takes five seconds, standing at the door is more efficient than the overhead of falling asleep and being woken up.

### Memory Visibility
Spinlocks and latches rely heavily on memory consistency models. When a lock is released, the changes made within the critical section must be made visible to the next thread that acquires the lock. This is achieved through memory barriers (fences) that prevent the compiler and the CPU from reordering instructions across the lock boundary.

## Standard Model
The standard model for a spinlock involves three distinct phases:

1.  **Acquisition Phase:** The thread attempts an atomic operation (e.g., TAS) to change the lock state from `FREE` to `HELD`.
2.  **Spin Phase:** If the acquisition fails, the thread enters a loop. In a "pure" spinlock, this loop continues indefinitely until the lock is acquired.
3.  **Release Phase:** The holder of the lock atomically sets the state back to `FREE`, typically accompanied by a memory barrier to ensure all preceding writes are committed.

### Latches vs. Locks
In the standard model of systems design (particularly in database internals), a distinction is made between latches and locks:
*   **Latches:** Protect physical data structures (e.g., pages in memory, hash table buckets). They are held for very short durations and have no deadlock detection.
*   **Locks:** Protect logical objects (e.g., rows, tables). They are held for the duration of a transaction and include complex queuing and deadlock detection logic.

## Common Patterns

### Test-and-Test-and-Set (TTAS)
To reduce bus traffic in multi-core systems, the TTAS pattern performs a standard "read" of the lock variable before attempting an expensive atomic "write." This keeps the cache line in a "Shared" state rather than constantly invalidating it across cores.

### Exponential Backoff
To prevent "thundering herd" problems and reduce interconnect contention, threads may introduce a delay between spin iterations. This delay increases exponentially if the lock remains unavailable.

### Ticket Spinlocks
This pattern ensures fairness (FIFO) by assigning a "ticket number" to each thread. Threads spin until their ticket number matches the "now serving" counter. This prevents starvation.

## Anti-Patterns

### Long Critical Sections
Holding a spinlock while performing I/O, page faults, or calling complex functions is a primary anti-pattern. Because the spinning thread consumes 100% of a CPU core, long-held spinlocks degrade system-wide performance.

### Priority Inversion
A low-priority thread holds a spinlock, and a high-priority thread spins on it. If the low-priority thread is preempted by a medium-priority thread, the high-priority thread may spin indefinitely.

> [!CAUTION]
> Never use a spinlock on a single-core (uniprocessor) system without disabling interrupts. If the thread holding the lock is preempted by the thread wanting the lock, the second thread will spin forever, as the first thread can never resume to release it.

## Edge Cases

### Recursive Acquisition
Standard spinlocks are non-recursive. If a thread attempts to acquire a spinlock it already holds, it will deadlock with itself, spinning forever waiting for a release that will never happen.

### Virtualized Environments
In virtual machines, a "Lock Holder Preemption" (LHP) can occur. If the hypervisor preempts a virtual CPU (vCPU) that is currently holding a spinlock, other vCPUs may waste millions of cycles spinning for a lock that cannot be released until the holder is rescheduled.

### Nested Latches
Acquiring multiple latches simultaneously requires a strict, global ordering (hierarchy) to prevent circular wait conditions, as latches typically do not have built-in deadlock detection mechanisms.

## Related Topics
*   **21. Mutexes and Semaphores:** High-level blocking primitives.
*   **22. Lock-Free Data Structures:** Techniques to avoid locks entirely using atomic operations.
*   **23. Memory Barriers and Fences:** The underlying hardware mechanisms for visibility.
*   **24. Cache Coherency Protocols:** How CPUs manage shared data (MESI/MOESI).

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-11 | Initial AI-generated canonical documentation |