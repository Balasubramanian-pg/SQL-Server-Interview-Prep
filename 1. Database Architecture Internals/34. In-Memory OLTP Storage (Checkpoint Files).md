# 34. In Memory OLTP Storage (Checkpoint Files)

Canonical documentation for 34. In Memory OLTP Storage (Checkpoint Files). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of In-Memory OLTP Storage (Checkpoint Files) is to provide a persistent, durable representation of data that resides primarily in volatile memory. While In-Memory engines optimize for high-concurrency and low-latency RAM access, they must adhere to ACID propertiesâ€”specifically Durability. 

Checkpoint files address the problem of data loss during system restarts or failures by maintaining a non-volatile record of memory-resident state. Unlike traditional page-based storage, checkpoint files are designed for streaming, sequential I/O to minimize the performance impact on the high-speed memory engine.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural pattern of Data and Delta file pairs.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * The architecture of Checkpoint File Pairs (CFP).
> * The mechanism of Data and Delta file separation.
> * The lifecycle of checkpoint files (creation, transition, and merging).
> * Recovery processes using checkpoint files.

> [!WARNING]
> **Out of scope:**
> * Specific transaction log (WAL) implementation details.
> * Memory-optimized table schema definitions.
> * Hardware-specific storage configuration (RAID levels, NVMe specs).

## Definitions
| Term | Definition |
|------|------------|
| Checkpoint File Pair (CFP) | A logical grouping consisting of one Data file and one Delta file. |
| Data File | A persistent file containing inserted rows within a specific timestamp range. |
| Delta File | A persistent file containing identifiers (tombstones) for rows in the corresponding Data file that have been deleted. |
| Merge Process | A background operation that combines multiple adjacent CFPs into a single pair to reclaim space and improve recovery time. |
| Streaming I/O | A method of writing data sequentially to disk, avoiding the overhead of random-access page updates. |
| Root File | A metadata file that tracks the inventory and state of all active checkpoint files. |

## Core Concepts
The fundamental idea behind In-Memory OLTP storage is the decoupling of the in-memory data format from the on-disk persistence format.

### Append-Only Architecture
Checkpoint files are append-only. Once a Data or Delta file is closed, it is never modified. This ensures that the storage engine utilizes sequential I/O, which is significantly faster than the random I/O required by traditional B-Tree page updates.

### The Data/Delta Split
To manage updates and deletions without modifying existing files:
1.  **Inserts:** New rows are written to the current Data file.
2.  **Deletes:** Instead of removing a row from a Data file, the row's unique identifier is written to the corresponding Delta file.
3.  **Updates:** An update is treated as a Delete (recorded in the Delta file) followed by an Insert (recorded in a new Data file).

> [!TIP]
> Think of a Data file as a "list of arrivals" and a Delta file as a "list of departures." To know who is currently in the building, you subtract the departures from the arrivals.

## Standard Model
The standard model for In-Memory storage utilizes a continuous stream of Checkpoint File Pairs (CFPs) managed by a background controller.

1.  **Pre-allocation:** The system maintains a pool of empty files to prevent latency spikes during file creation.
2.  **Active Phase:** As transactions commit, the engine streams data to the current CFP.
3.  **Closing Phase:** Once a CFP reaches a size threshold (e.g., 128MB), it is marked as "Closed" and a new CFP is opened.
4.  **Recovery Phase:** During a system restart, the engine reads all active CFPs. It loads the Data files into memory but filters out any rows identified in the corresponding Delta files.
5.  **Merge Phase:** To prevent "file bloat," a background process identifies CFPs where a significant percentage of rows have been deleted (via the Delta file) and merges the remaining "live" rows into a new, consolidated CFP.

## Common Patterns
*   **Parallel Recovery:** Modern engines read multiple CFPs simultaneously across different I/O channels to populate RAM as quickly as possible during startup.
*   **Continuous Checkpointing:** Rather than a massive "stop-the-world" flush, the system continuously streams committed changes to the current CFP.
*   **Filtering at Load:** During recovery, the engine applies the Delta file as a filter against the Data file in a single pass.

## Anti-Patterns
*   **Small File Thresholds:** Setting checkpoint file sizes too small leads to excessive file handles and metadata overhead.
*   **Manual File Deletion:** Attempting to manage or delete checkpoint files via the OS file system rather than the database engine will lead to catastrophic data loss or corruption.
*   **Synchronous Disk Commits:** Forcing the memory engine to wait for a physical disk flush on every small transaction negates the performance benefits of In-Memory OLTP.

> [!CAUTION]
> Avoid placing checkpoint files on high-latency storage. While the I/O is sequential, the speed of the merge process and the duration of system recovery are directly bound by the storage throughput.

## Edge Cases
*   **Storage Exhaustion:** If the disk hosting checkpoint files runs out of space, the In-Memory engine may stop accepting write transactions, even if RAM is plentiful, to ensure durability cannot be compromised.
*   **Massive Deletions:** A workload that deletes millions of rows without new inserts can lead to large Delta files. If the merge process cannot keep up, recovery time may increase because the engine must process long lists of "tombstones."
*   **Corrupt Delta Files:** If a Data file is intact but its corresponding Delta file is lost or corrupt, the system may "resurrect" deleted data, leading to logical inconsistency.

## Related Topics
*   **Write-Ahead Logging (WAL):** The mechanism that captures changes before they are hardened into checkpoint files.
*   **Garbage Collection (In-Memory):** The process of cleaning up row versions in RAM, which is distinct from the disk-based Merge process.
*   **Recovery Time Objective (RTO):** The business metric directly impacted by the efficiency and size of checkpoint files.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-11 | Initial AI-generated canonical documentation |