# 23. Full Text Indexing Architecture

Canonical documentation for 23. Full Text Indexing Architecture. This document defines concepts, terminology, and standard usage.

## Purpose
Full Text Indexing (FTI) architecture exists to solve the problem of efficient, linguistic-based retrieval of unstructured or semi-structured text data. Traditional relational indexing (such as B-Trees) is optimized for exact matches, range scans, and prefix matching. However, these structures fail when users need to search for keywords within large bodies of text, handle morphological variations of words, or rank results by relevance.

The architecture provides a mechanism to transform "human-readable" text into a "machine-searchable" mathematical representation, typically an inverted index, allowing for sub-second responses across terabytes of textual information.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the underlying mechanics of text analysis and retrieval systems.

## Scope
The scope of this documentation covers the structural components required to build, maintain, and query a full-text search system.

> [!IMPORTANT]
> **In scope:**
> * The lifecycle of a document from ingestion to index.
> * Text analysis pipelines (tokenization, normalization, filtering).
> * The structure and function of Inverted Indexes.
> * Scoring and relevance theory.
> * Query-time processing and expansion.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Elasticsearch, Lucene, Solr, or Cloud-native search services).
> * Hardware-level storage optimization (e.g., SSD vs. HDD configurations).
> * User Interface (UI) design for search bars.

## Definitions
| Term | Definition |
|------|------------|
| Document | The basic unit of information that can be indexed and searched. |
| Corpus | The entire collection of documents within a search system. |
| Token | A single unit of text (usually a word) produced during the analysis process. |
| Inverted Index | A data structure mapping tokens to the documents (and positions) where they occur. |
| Stemming | The process of reducing a word to its base or root form (e.g., "running" to "run"). |
| Lemmatization | A linguistic process that identifies the canonical form of a word based on its intended meaning. |
| Stop Words | Common words (e.g., "the", "is", "at") that are often filtered out to save space and improve relevance. |
| TF-IDF | Term Frequency-Inverse Document Frequency; a numerical statistic reflecting how important a word is to a document in a corpus. |
| BM25 | Best Matching 25; a ranking function used to estimate the relevance of documents to a given search query. |

## Core Concepts

### The Inverted Index
The cornerstone of FTI architecture is the Inverted Index. Unlike a book's table of contents (which lists chapters and then content), an inverted index is like the index at the back of a textbook. It lists every unique word and points to the locations where those words appear.

### The Analysis Pipeline
Before text is stored in the index, it must pass through an analysis pipeline. This pipeline consists of:
1.  **Character Filtering:** Removing HTML tags or converting special characters.
2.  **Tokenization:** Splitting the string into individual terms (tokens).
3.  **Token Filtering:** Applying transformations like lowercasing, stop-word removal, and stemming.

> [!TIP]
> Think of the analysis pipeline as a "refinery" that converts raw, messy text into standardized, searchable units.

### Relevance and Scoring
Search is not binary (Yes/No). It is probabilistic. The architecture must calculate a "score" for every document matching a query. This score is influenced by:
*   **Term Frequency:** How often the term appears in the document.
*   **Inverse Document Frequency:** How rare the term is across the entire corpus.
*   **Field Length:** Shorter fields where the term appears are often weighted more heavily than long fields.

## Standard Model
The standard model for Full Text Indexing follows a decoupled "Write-Read" architecture:

1.  **The Indexing Path (Write):**
    *   **Ingestion:** Raw data is received.
    *   **Analysis:** The text is processed into tokens.
    *   **Indexing:** The inverted index is updated. In modern distributed systems, this often involves writing to an immutable "segment" that is later merged.

2.  **The Query Path (Read):**
    *   **Query Analysis:** The user's search string is passed through the *same* analysis pipeline used during indexing to ensure tokens match.
    *   **Lookup:** The system finds the tokens in the inverted index.
    *   **Merging/Intersection:** If multiple terms are searched, the system intersects the document lists.
    *   **Scoring:** Results are ranked.
    *   **Retrieval:** The top $N$ documents are returned to the user.

## Common Patterns

### N-Grams and Edge N-Grams
To support "search-as-you-type" or partial word matching, text is broken into overlapping chunks of $N$ characters. For example, "search" becomes `["se", "sea", "sear", "searc"]`.

### Synonym Expansion
Mapping different terms to the same concept (e.g., "cellphone" and "mobile") at either index time or query time to increase recall.

### Sharding and Replication
Distributing the index across multiple nodes to handle massive datasets and provide high availability.

## Anti-Patterns

### Using FTI for Primary Key Lookups
Using a full-text engine to retrieve data by a known unique ID is inefficient compared to a standard Key-Value store or B-Tree index.

### Indexing Binary Data Directly
Attempting to index PDFs, Images, or Word docs without an extraction layer (like OCR or text extraction) results in "garbage" tokens.

### Over-Analysis
Applying too many filters (e.g., aggressive stemming) can lead to "false positives" where unrelated words are reduced to the same root, destroying search precision.

> [!CAUTION]
> Avoid using Full Text Indexing for frequently updated "volatile" data. The overhead of re-indexing and segment merging can lead to significant I/O bottlenecks.

## Edge Cases

### CJK Languages
Chinese, Japanese, and Korean languages do not use whitespace to separate words. FTI architecture for these languages requires specialized "Dictionary-based" or "HMM-based" (Hidden Markov Model) tokenizers to identify word boundaries.

### Diacritics and Accents
In many languages, `e`, `é`, and `è` are distinct. An architecture must decide whether to "fold" these into a base character (ASCII folding) to increase recall or keep them distinct for precision.

### Extremely Large Documents
Indexing a 500-page book as a single document can dilute relevance scores. A common strategy is "shingling" or breaking the document into smaller, overlapping logical sections (paragraphs or pages).

## Related Topics
*   **Natural Language Processing (NLP):** Advanced techniques for understanding intent.
*   **Vector Databases:** Using embeddings for semantic search rather than keyword matching.
*   **Information Retrieval (IR) Theory:** The mathematical foundation of search.
*   **Database Indexing:** Comparison between B-Trees, LSM Trees, and Inverted Indexes.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-13 | Initial AI-generated canonical documentation |