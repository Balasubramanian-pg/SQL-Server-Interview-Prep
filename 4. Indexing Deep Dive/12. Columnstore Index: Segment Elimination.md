# 12. Columnstore Index: Segment Elimination

Canonical documentation for [12. Columnstore Index: Segment Elimination](4. Indexing Deep Dive/12. Columnstore Index: Segment Elimination.md). This document defines concepts, terminology, and standard usage.

## Purpose
Segment elimination is a performance optimization technique used in columnstore architectures to minimize I/O and CPU consumption during query execution. By utilizing metadata stored at the storage layer, the query engine can identify and bypass entire blocks of data (segments) that do not contain values relevant to a query's filter criteria. This mechanism transforms a full table scan into a targeted scan, significantly reducing the volume of data read from disk or memory.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural principles of columnar data skipping.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Metadata-driven data skipping logic.
> * The relationship between data distribution (clustering) and elimination efficiency.
> * The impact of predicates on segment selection.
> * Theoretical boundaries of min/max pruning.

> [!WARNING]
> **Out of scope:**
> * Specific vendor-specific syntax for creating indexes.
> * Hardware-level storage optimizations (e.g., NVMe-specific protocols).
> * Row-level filtering logic that occurs after a segment has been read.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Segment | A compressed unit of data representing a specific column within a Row Group. |
| Row Group | A logical grouping of rows (typically in the range of hundreds of thousands to millions) that are compressed into columnar format together. |
| Metadata | Information stored alongside segments, such as minimum and maximum values, used to evaluate predicates without reading the actual data. |
| Predicate | A logical expression in a query (e.g., `WHERE Date > '2023-01-01'`) used to filter results. |
| SARGability | The ability of a query predicate to be used effectively by the engine to skip data (Search ARGumentable). |

## Core Concepts
The fundamental principle of segment elimination is the "Min/Max" check. For every segment in a columnstore index, the system maintains high-level statistics, specifically the minimum and maximum values present in that segment.

When a query is executed with a filter, the query optimizer compares the filter range against the segment's Min/Max range:
1.  **Disjoint:** If the filter range (e.g., `Value > 100`) is entirely outside the segment range (e.g., `Min: 10, Max: 50`), the segment is **eliminated**.
2.  **Contained/Overlapping:** If there is any potential for the segment to contain relevant data, the segment is **read and decompressed**.

> [!TIP]
> Think of segment elimination like the index of a multi-volume encyclopedia. If you are looking for "Zebra," and Volume 1 is labeled "A-M," you can skip Volume 1 entirely without opening a single page.

## Standard Model
The standard model for segment elimination follows a three-step process during the query execution plan:

1.  **Predicate Analysis:** The engine identifies constant-based filters in the query.
2.  **Metadata Comparison:** The engine scans the system metadata tables (which are much smaller than the actual data) to compare the filters against the Min/Max boundaries of all segments for the referenced columns.
3.  **Execution Skip:** The storage engine generates a list of "qualified" segments. Only these segments are fetched from storage, decompressed, and passed to the next operator in the execution tree.

## Common Patterns
*   **Time-Series Ordering:** Data is naturally inserted in chronological order. This creates segments with non-overlapping date ranges, allowing for near-perfect elimination on date-based queries.
*   **Clustered Columnstore Sorting:** Explicitly sorting data by a high-cardinality column (like `CustomerID` or `TransactionID`) before or during index creation to ensure that specific values are concentrated in as few segments as possible.
*   **Partitioning Synergy:** Using table partitioning in conjunction with segment elimination to provide two layers of data skipping (partition pruning followed by segment elimination).

## Anti-Patterns
*   **Randomized Data Distribution:** If data is inserted in a completely random order (e.g., using a GUID as a primary key without sorting), the Min/Max ranges for every segment will likely overlap, covering the entire range of possible values. This renders segment elimination useless.
*   **Over-fragmentation:** Creating segments that are too small (e.g., only a few thousand rows) increases metadata overhead and can lead to "death by a thousand I/Os."
*   **Non-SARGable Predicates:** Using functions on columns in the `WHERE` clause (e.g., `WHERE YEAR(TransactionDate) = 2023`) often prevents the engine from performing Min/Max comparisons, forcing a full scan.

> [!CAUTION]
> Avoid frequent small "trickle" inserts into a columnstore index. This often results in many small, poorly compressed segments with wide-ranging metadata, which severely degrades elimination performance.

## Edge Cases
*   **NULL Values:** Systems handle NULLs differently; some treat them as the "minimum" possible value, while others maintain a separate bitmask. If a segment contains even one NULL, it may affect how the Min/Max range is interpreted for certain predicates.
*   **Soft Deletes:** If a segment has many rows marked as deleted but not yet physically removed (via a merge or rebuild), the Min/Max metadata still reflects the deleted rows, potentially preventing elimination even if the "active" rows are outside the filter range.
*   **Data Type Precision:** In some implementations, floating-point precision or collation settings for strings can lead to "false positives" where a segment is read even if it contains no matching data, due to rounding or comparison complexities in the metadata layer.

## Related Topics
*   **Columnstore Compression:** The relationship between how data is packed and how metadata is generated.
*   **Row Group Management:** The lifecycle of row groups (Open, Closed, Compressed).
*   **Predicate Pushdown:** The broader category of moving filtering logic as close to the data as possible.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-13 | Initial AI-generated canonical documentation |