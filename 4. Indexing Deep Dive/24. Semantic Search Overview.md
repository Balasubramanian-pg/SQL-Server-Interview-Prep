# 24. Semantic Search Overview

Canonical documentation for [24. Semantic Search Overview](4. Indexing Deep Dive/24. Semantic Search Overview.md). This document defines concepts, terminology, and standard usage.

## Purpose
Semantic search is a data retrieval methodology that prioritizes the intent and contextual meaning behind a search query rather than relying solely on literal keyword matching (lexical search). It addresses the "lexical gap"â€”the discrepancy between the specific words a user types and the actual information they seek.

By representing data in a high-dimensional mathematical space, semantic search enables systems to understand synonyms, polysemy (words with multiple meanings), and complex relationships between concepts, thereby providing more relevant results in unstructured data environments.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the mathematical and logical foundations of semantic retrieval rather than specific database engines.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Mathematical representation of meaning (Embeddings)
> * Vector space modeling and distance metrics
> * Retrieval architectures (Bi-encoders and Cross-encoders)
> * The relationship between semantic search and Natural Language Processing (NLP)

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Pinecone, Weaviate, Milvus)
> * Detailed tutorials on training specific Large Language Models (LLMs)
> * Front-end UI/UX design for search interfaces

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **Embedding** | A numerical representation of a piece of information (text, image, audio) as a high-dimensional vector. |
| **Vector Space** | A mathematical space where the distance between points represents the degree of semantic similarity between items. |
| **Cosine Similarity** | A metric used to measure how similar two vectors are by calculating the cosine of the angle between them. |
| **Dense Retrieval** | A search method using continuous low-dimensional vectors (embeddings) to find relevant documents. |
| **Sparse Retrieval** | Traditional keyword-based search (e.g., BM25) that relies on word frequency and exact matches. |
| **Bi-Encoder** | A model architecture that processes the query and the document independently into vectors for fast comparison. |
| **Cross-Encoder** | A model architecture that processes the query and document simultaneously to produce a similarity score, offering higher accuracy but lower speed. |

## Core Concepts
Semantic search operates on the principle that "meaning" can be mapped to coordinates in a multi-dimensional space.

### Latent Representation
In traditional search, the word "bank" is treated as a string of characters. In semantic search, the system uses context to determine if "bank" refers to a financial institution or the side of a river. This is achieved through latent representation, where the model captures hidden features of the language.

> [!TIP]
> Think of a vector space as a massive library where books aren't organized by title or author, but by the "vibe" or "essence" of their content. Books about "sailing" and "ocean navigation" would be physically close to each other, even if they don't share a single word in their titles.

### Dimensionality
The "richness" of a semantic model is often determined by its dimensionality. A vector might have 768 or 1536 dimensions. Each dimension represents an abstract feature of the data (e.g., "gender," "royalty," "temperature"), though these features are rarely human-interpretable.

## Standard Model
The generally accepted model for a semantic search pipeline follows these stages:

1.  **Document Ingestion:** Raw text is cleaned and broken into manageable "chunks."
2.  **Embedding Generation:** A pre-trained transformer model converts these chunks into dense vectors.
3.  **Indexing:** Vectors are stored in a specialized data structure (Vector Index) designed for Approximate Nearest Neighbor (ANN) lookups.
4.  **Query Processing:** The user's natural language query is converted into a vector using the same embedding model used for the documents.
5.  **Similarity Search:** The system calculates the distance between the query vector and the indexed vectors.
6.  **Result Ranking:** The most "proximal" documents are returned to the user, often sorted by a similarity score.

## Common Patterns

### Hybrid Search
Combining dense retrieval (semantic) with sparse retrieval (keyword). This ensures that the system captures both the "meaning" of the query and specific technical terms or identifiers that semantic models might overlook.

### Retrieval-Augmented Generation (RAG)
Using semantic search as a "lookup" mechanism for LLMs. The search engine finds relevant context, which is then fed into the LLM to generate a grounded, factual response.

### Two-Stage Retrieval
A pattern where a fast Bi-Encoder retrieves the top 100 candidates, and a more computationally expensive Cross-Encoder "re-ranks" those 100 to find the absolute best matches.

## Anti-Patterns
Common mistakes or discouraged practices.

*   **The "SKU" Problem:** Using semantic search for exact alphanumeric identifiers (e.g., part numbers like "XJ-900"). Semantic models often "smooth over" these unique strings, leading to poor precision.
*   **Ignoring Chunking Strategy:** Creating embeddings for documents that are too long (losing specificity) or too short (losing context).
*   **Model Mismatch:** Using one model to embed the documents and a different model to embed the query. This results in vectors existing in two different, incompatible mathematical spaces.

> [!CAUTION]
> Avoid treating semantic search as a replacement for all keyword search. Semantic search is probabilistic, not deterministic; it may fail to find a document that contains the exact word the user typed if the model deems the "intent" to be different.

## Edge Cases
*   **Out-of-Vocabulary (OOV) Terms:** New slang, brand names, or highly technical jargon not present in the model's training data can lead to "hallucinated" vector positions.
*   **Negation:** Many semantic models struggle with negation (e.g., "movies that are NOT comedies"). The vector for "not a comedy" may still end up close to the vector for "comedy."
*   **Short Queries:** Single-word queries (e.g., "Apple") lack the context necessary for the model to distinguish between the fruit and the technology company.

## Related Topics
*   **Vector Databases:** The storage layer for semantic vectors.
*   **Natural Language Processing (NLP):** The broader field encompassing the models used for embeddings.
*   **Information Retrieval (IR):** The academic discipline governing search science.
*   **Large Language Models (LLMs):** The primary drivers of modern semantic embedding technology.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-13 | Initial AI-generated canonical documentation |