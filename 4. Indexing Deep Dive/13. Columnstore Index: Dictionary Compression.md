# 13. Columnstore Index: Dictionary Compression

Canonical documentation for [13. Columnstore Index: Dictionary Compression](4. Indexing Deep Dive/13. Columnstore Index: Dictionary Compression.md). This document defines concepts, terminology, and standard usage.

## Purpose
Dictionary compression exists to minimize the storage footprint of columnar data by eliminating redundancy within a dataset. In column-oriented storage, data often contains repetitive values (e.g., country names, product categories, or status codes). Instead of storing these repetitive strings or large values directly in every row, dictionary compression replaces them with smaller, fixed-width integer keys (tokens) that point to a unique set of values stored in a separate structure.

This mechanism addresses the problem of I/O bottlenecks and memory pressure by increasing data density, allowing more information to be processed in a single CPU cycle and reducing the physical storage required.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the underlying architectural principles of dictionary-based encoding in columnar systems.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * The architectural relationship between dictionaries and data streams.
> * Encoding and decoding mechanisms.
> * Impact of cardinality on compression efficiency.
> * Memory management strategies for dictionary structures.

> [!WARNING]
> **Out of scope:**
> * Specific vendor-specific syntax for creating indexes.
> * Hardware-specific optimizations (e.g., SIMD instructions), though the concepts remain relevant.
> * Row-based compression techniques (e.g., page or row compression) that do not utilize columnar dictionaries.

## Definitions
| Term | Definition |
|------|------------|
| Dictionary | A data structure containing a sorted or hashed collection of unique values found within a column. |
| Token / Key | A short, fixed-width integer used to represent a specific value within the dictionary. |
| Cardinality | The measure of unique values in a column; high cardinality implies many unique values, while low cardinality implies many repetitions. |
| Bit-Packing | The process of using the minimum number of bits required to represent the range of tokens in a dictionary. |
| Reference Stream | The actual column data stored as a sequence of tokens rather than the original values. |
| Global Dictionary | A dictionary that spans multiple segments or row groups within a columnstore. |
| Local Dictionary | A dictionary restricted to a single segment or row group. |

## Core Concepts
The fundamental idea of dictionary compression is the decoupling of data values from their occurrences. By transforming a variable-width or large-value column into a stream of small, uniform integers, the system achieves two goals: storage reduction and computational efficiency.

**The Mapping Process**
When data is ingested, the engine identifies unique values. Each unique value is assigned a numeric identifier. The original data is then discarded in favor of these identifiers. During query execution, the engine can often perform filters and joins directly on the numeric tokens without ever "decompressing" the dictionary until the final result set is required.

> [!TIP]
> Think of dictionary compression like a library's card catalog. Instead of writing the full description of a book on every checkout slip, the librarian uses a short "Call Number." The call number (token) is much faster to write and takes up less space on the slip (storage), but it points back to the full record in the catalog (dictionary) when the details are needed.

## Standard Model
The standard model for dictionary compression in a columnstore environment consists of a two-tiered architecture:

1.  **The Dictionary Store:** A metadata structure that maps `Token -> Value`. This is typically sorted to allow for range scans and binary searches.
2.  **The Encoded Data Stream:** A contiguous block of memory or disk space containing the tokens.

**Encoding Workflow:**
*   **Analysis:** The system scans a segment of data to determine the number of unique entries.
*   **Dictionary Construction:** Unique values are extracted and stored.
*   **Bit-Width Calculation:** If a dictionary contains 256 unique values, the system determines that each token only requires 8 bits (1 byte).
*   **Substitution:** The original values are replaced by the bit-packed tokens.

## Common Patterns
*   **Sorted Dictionaries:** Storing dictionary values in alphabetical or numerical order to allow for "Predicate Pushdown." This allows the engine to evaluate conditions (like `City > 'N'`) by finding the token range in the dictionary first.
*   **Multi-Level Dictionaries:** Using a local dictionary for specific row groups and a global dictionary for shared values across the entire table to maximize the compression ratio.
*   **Fallback to Raw:** If the number of unique values exceeds a certain threshold (high cardinality), the system may abandon dictionary compression for that specific segment to avoid the overhead of a massive dictionary.

## Anti-Patterns
*   **Dictionary Pressure:** Attempting to use dictionary compression on columns with near-unique values (e.g., primary keys, timestamps with millisecond precision, or GUIDs). This results in a dictionary almost as large as the data itself, plus the overhead of the token stream.
*   **Small Row Groups:** Creating very small segments of data. Dictionary compression relies on finding repetitions; if the segment is too small, the "cost" of storing the dictionary metadata outweighs the savings of the compressed tokens.
*   **Frequent Updates:** Columnstore indexes are optimized for read-heavy workloads. Frequent updates to a dictionary-compressed column require constant rebuilding of the dictionary and re-encoding of tokens, leading to significant CPU overhead.

> [!CAUTION]
> Avoid using dictionary-compressed columns for high-entropy data like encrypted strings or random hashes. These values do not repeat, rendering the dictionary mechanism counter-productive and increasing total storage size.

## Edge Cases
*   **Null Handling:** Systems must decide whether `NULL` is a reserved token in the dictionary or handled via a separate bitmask (null bitmap).
*   **Dictionary Overflow:** When the number of unique values in a segment exceeds the maximum value of the chosen bit-width (e.g., more than 65,535 values for a 16-bit token), the system must either promote the bit-width or split the segment.
*   **Data Type Changes:** If a column's data type is altered, the entire dictionary and all associated reference streams must be recalculated, as the token-to-value mapping is strictly typed.

## Related Topics
*   **Run-Length Encoding (RLE):** Often used in conjunction with dictionary compression to further compress sequences of identical tokens.
*   **Bit-Packing:** The underlying method of storing tokens using the fewest bits possible.
*   **Predicate Pushdown:** The optimization where filters are applied to the dictionary tokens rather than the decompressed data.
*   **Columnar Segment Elimination:** Using dictionary metadata to skip entire blocks of data that do not contain relevant values.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-13 | Initial AI-generated canonical documentation |