# 52. Table Variables vs Temp Tables

### 1. The Anchor
* **One-Sentence Pitch:** Table variables and temporary tables are both mechanisms for storing transient data, but they differ fundamentally in scope, transaction behavior, and how the query optimizer handles them.
* **Category:** Technical Skill / Database Engineering

### 2. The Mechanics
* **Key Detail A: Scope and Lifecycle:** Table variables (`DECLARE @T TABLE`) are scoped strictly to the batch, stored procedure, or function in which they are declared and are automatically cleaned up at the end of the batch. Temporary tables (`CREATE TABLE #T`) are scoped to the entire session and persist until the session is closed or they are explicitly dropped.
* **Key Detail B: Statistics and Performance:** Temporary tables maintain distribution statistics, allowing the SQL engine to generate optimized execution plans for large datasets. Table variables (historically) do not maintain statistics, often leading the optimizer to assume a fixed row count of 1, which can cause significant performance degradation in complex joins.
* **The Logic:** This distinction exists to balance overhead versus optimization. Table variables are designed for small datasets where the overhead of creating statistics and triggering query recompilations would outweigh the benefits. Temporary tables are designed for larger, more complex datasets where the optimizer needs accurate data distribution info to choose the correct join algorithms (e.g., Hash Match vs. Nested Loops).

> [!IMPORTANT]
> While table variables are often thought of as "in-memory," they are still backed by `tempdb` physical storage if they exceed a certain size. The primary performance difference is not memory vs. disk, but rather the presence of statistics and the impact on query recompilation.

### 3. The Evidence
* **Context (CAR/STAR):** A legacy financial reporting procedure was experiencing intermittent timeouts when processing month-end batches of approximately 250,000 records.
* **Action:** I identified that the procedure was using a table variable to store intermediate calculation results. Because the optimizer assumed only 1 row was in the table variable, it chose a Nested Loop join for a 250k-row dataset. I refactored the procedure to use a temporary table (`#Temp`) and added a clustered index on the primary join key.
* **Result:** The query optimizer switched to a Hash Match join, reducing the execution time from 120 seconds to under 8 seconds. This change also reduced the total CPU utilization on the database server during peak reporting hours by 15%.

> [!TIP]
> Use Table Variables for small datasets (typically < 1,000 rows) where you want to avoid the overhead of recompilations. Use Temporary Tables for large datasets or when you need to create non-primary key indexes to support complex queries.

### 4. The Interview "Pivot"
* **Triggers:** "How do you handle performance tuning in SQL?", "What is the difference between `@` and `#` tables?", or "Tell me about a time you optimized a slow-running process."
* **The Connection:** This topic allows me to demonstrate a deep understanding of database internals and the SQL Server Query Optimizer. It proves that I don't just write code that works, but code that is architected for scale and performance by choosing the right tool for the specific data volume.

> [!NOTE]
> In SQL Server 2019 and later, "Table Variable Deferred Compilation" helps mitigate the "1-row estimate" issue, but temporary tables remain the standard for complex data manipulation due to their support for explicit indexes and statistics.