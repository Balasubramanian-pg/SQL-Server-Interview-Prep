# 51. Batch Processing vs Cursor Logic

### 1. The Anchor
* **One-Sentence Pitch:** Batch processing leverages set-based operations to handle large data volumes efficiently by minimizing overhead, while cursor logic employs row-by-row iteration for complex, granular record manipulation.
* **Category:** Technical Skill / Database Architecture & Performance Tuning

### 2. The Mechanics
* **Key Detail A: Set-Based (Batch) Execution:** Batch processing treats data as a collective unit. It utilizes the database engine's optimizer to execute operations across thousands of rows simultaneously. This is typically achieved through declarative SQL (e.g., `UPDATE...WHERE`) or by "chunking" data into manageable subsets to prevent transaction log exhaustion and locking.
* **Key Detail B: Procedural (Cursor) Iteration:** Cursors establish a pointer to a specific row within a result set. The system fetches one row, performs logic, and moves to the next. This is often referred to as RBAR (Row By Agonizing Row). While it allows for complex conditional logic that is difficult to express in pure SQL, it incurs massive overhead due to repeated context switching between the storage engine and the execution engine.
* **The Logic:** The fundamental goal is to balance **throughput** against **complexity**. Batch processing is the default choice for performance because it reduces I/O requests and maximizes the efficiency of the database's internal execution plan. Cursors are a "last resort" used only when a task requires sequential dependencies where the outcome of row $N$ dictates the processing of row $N+1$.

> [!TIP]
> When batching, always use a "High Water Mark" (like an ID or Timestamp) rather than `OFFSET` for pagination. `OFFSET` becomes increasingly slow as the scan depth increases, whereas a filtered index remains constant in performance.

### 3. The Evidence
* **Context (CAR/STAR):** A financial services platform was experiencing 4-hour delays in nightly interest calculations. The legacy system used a cursor to fetch every active account, calculate interest in application code, and write back individual updates.
* **Action:** I refactored the process into a batched, set-based operation. I implemented a "Staging-to-Production" pattern where interest was calculated using a single vectorized SQL statement into a temporary table. I then applied these updates to the main ledger in chunks of 10,000 records using a keyed-seek approach to avoid table-wide locks.
* **Result:** The processing time dropped from 4 hours to 12 minutes. Database CPU utilization during the window decreased by 60%, and the risk of deadlocks—which previously occurred twice a week—was eliminated.

> [!IMPORTANT]
> Batching is not just about speed; it is about concurrency. Large, single-transaction batches can lock tables for extended periods. Breaking batches into smaller, committed chunks allows other processes to access the data between iterations.

### 4. The Interview "Pivot"
* **Triggers:** "How do you optimize a slow-running data migration?", "What is your approach to handling millions of records?", or "When would you use a cursor instead of a standard JOIN?"
* **The Connection:** This topic allows me to demonstrate that I don't just write code that works; I write code that scales. By explaining the trade-offs between batching and cursors, I prove I understand database internals, resource contention, and the importance of writing "database-friendly" logic to ensure system stability under high load.

> [!WARNING]
> Beware of "Hidden Cursors." Some ORMs (Object-Relational Mappers) default to row-by-row processing when you use a `foreach` loop over a query result. Always verify the generated SQL to ensure it is truly batched.