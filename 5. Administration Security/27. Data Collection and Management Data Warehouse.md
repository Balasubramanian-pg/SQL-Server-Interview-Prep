# 27. Data Collection and Management Data Warehouse

Canonical documentation for [27. Data Collection and Management Data Warehouse](5. Administration Security/27. Data Collection and Management Data Warehouse.md). This document defines concepts, terminology, and standard usage.

## Purpose
The Data Warehouse (DW) serves as the central repository for integrated data from one or more disparate sources. Its primary purpose is to facilitate analytical processing, reporting, and data-driven decision-making by providing a structured, historical, and consistent view of an organization's information assets. 

In the context of data collection and management, the warehouse acts as the destination for processed data, transforming raw operational inputs into curated information. It addresses the problem of "data silos" by consolidating information into a unified environment optimized for complex queries and long-term trend analysis rather than transactional throughput.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural and management principles of data warehousing rather than specific software platforms.

## Scope
This documentation covers the theoretical and structural foundations of data warehousing within a broader data management strategy.

> [!IMPORTANT]
> **In scope:**
> * Architectural layers (Staging, Integration, Presentation).
> * Data modeling methodologies (Dimensional, Normalized, Vault).
> * Data lifecycle management within the warehouse.
> * Metadata management and data lineage.
> * The distinction between ETL and ELT paradigms.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Snowflake, BigQuery, Redshift).
> * Physical hardware configurations or specific cloud provider pricing models.
> * Front-end Business Intelligence (BI) tool tutorials.

## Definitions
| Term | Definition |
|------|------------|
| ETL/ELT | Extract, Transform, Load (or Load, Transform); the processes used to move data from source systems to the warehouse. |
| OLAP | Online Analytical Processing; a category of software tools that provide analysis of data stored in a database. |
| Dimension | A structure that categorizes facts and measures in order to enable users to answer business questions. |
| Fact Table | The central table in a dimensional model containing the quantitative measures (facts) of a business process. |
| Schema-on-Write | The requirement to define the data structure before loading data into the warehouse. |
| Data Mart | A subset of a data warehouse, usually oriented to a specific business line or team. |
| Slowly Changing Dimension (SCD) | A technique for managing how the warehouse handles changes to dimension members over time. |

## Core Concepts
The Data Warehouse is defined by four primary characteristics: it is subject-oriented, integrated, time-variant, and non-volatile.

*   **Subject-Oriented:** Data is organized around key subjects (e.g., Customers, Sales, Products) rather than ongoing business operations.
*   **Integrated:** Data from multiple sources is standardized (e.g., converting different date formats or currency codes into a single standard).
*   **Time-Variant:** Data is identified with a particular time period, allowing for historical analysis and trend identification.
*   **Non-Volatile:** Once data is entered into the warehouse, it is not updated or deleted in the same way operational data is; it is preserved for historical record.

> [!TIP]
> Think of a Data Warehouse as a library. While a bookstore (Operational Database) focuses on the latest transactions and quick turnover, a library (Data Warehouse) focuses on categorizing, preserving, and making a vast history of information searchable for researchers.

## Standard Model
The standard model for a Data Warehouse typically follows a layered architecture to ensure data quality and traceability:

1.  **Staging Layer:** A landing zone for raw data extracted from source systems. Minimal transformation occurs here.
2.  **Integration Layer (ODS/EDW):** Data is cleansed, de-duplicated, and integrated. This layer often uses a normalized (3NF) or Data Vault structure to maintain a "single version of truth."
3.  **Presentation Layer:** Data is modeled for end-user consumption, typically using dimensional modeling (Star or Snowflake schemas).
4.  **Metadata Layer:** A directory or "data about data" that manages definitions, lineage, and business logic applied during the movement through layers.

## Common Patterns
*   **Star Schema:** A central fact table connected to multiple dimension tables. This is the most common pattern for performance and ease of use in BI tools.
*   **Snowflake Schema:** An extension of the star schema where dimension tables are normalized into multiple related tables.
*   **Data Vault:** A detail-oriented, historical tracking method that separates business keys (Hubs), relationships (Links), and descriptive attributes (Satellites). It is highly resilient to changes in source systems.
*   **Incremental Loading:** Only capturing and loading data that has changed since the last execution, reducing processing overhead.

## Anti-Patterns
*   **The Data Swamp:** Loading raw data into the warehouse without metadata, governance, or a clear schema, making the data undiscoverable or untrustworthy.
*   **Direct Source Querying:** Allowing analytical tools to query operational databases directly, which can degrade production performance and lacks historical context.
*   **Hard-Coding Logic:** Embedding business transformations directly into the loading scripts rather than managing them through a centralized metadata or transformation layer.
*   **Ignoring Granularity:** Mixing different levels of detail (e.g., daily sales and monthly targets) in the same fact table without proper handling.

> [!CAUTION]
> Avoid circular dependencies where the warehouse relies on downstream analytical outputs to populate its core integration layers. This creates a "feedback loop" that compromises data integrity.

## Edge Cases
*   **Late-Arriving Facts:** Scenarios where a transaction occurs on Day 1 but is not received by the warehouse until Day 5. This requires specific logic to update historical records without corrupting time-series analysis.
*   **Schema Evolution:** Handling structural changes in source systems (e.g., a new column added or a data type changed) without breaking existing warehouse pipelines.
*   **PII and Compliance:** Managing Personally Identifiable Information (PII) within a historical archive, specifically addressing "the right to be forgotten" (GDPR/CCPA) in an environment designed for non-volatility.
*   **Near Real-Time Requirements:** While warehouses are traditionally batch-oriented, modern requirements often demand "micro-batching" or streaming ingestion, blurring the line between the warehouse and operational systems.

## Related Topics
*   **Data Lake:** A storage repository that holds a vast amount of raw data in its native format until it is needed.
*   **Data Governance:** The overall management of the availability, usability, integrity, and security of data.
*   **Master Data Management (MDM):** A method used to define and manage the critical data of an organization to provide a single point of reference.
*   **Business Intelligence (BI):** The strategies and technologies used by enterprises for data analysis and business information.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-13 | Initial AI-generated canonical documentation |