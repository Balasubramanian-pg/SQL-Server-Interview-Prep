# 10. Handling Multiple Rows in Triggers

Canonical documentation for [10. Handling Multiple Rows in Triggers](7. Programming Objects/10. Handling Multiple Rows in Triggers.md). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of handling multiple rows in triggers is to ensure data integrity and business logic consistency when Data Manipulation Language (DML) operations affect more than a single record. In relational database systems, a single statement (such as a bulk `INSERT`, a filtered `UPDATE`, or a `DELETE` with a `WHERE` clause) can modify thousands or millions of rows simultaneously. Triggers must be architected to process these changes as sets rather than assuming a scalar, single-row context. This topic addresses the transition from row-based logic to set-based logic within the database's automated reactive layer.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the logical requirements of set-based trigger design.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Set-based processing logic.
> * Management of transition states (Before/After) for multiple records.
> * Performance considerations for batch operations within triggers.
> * Logical consistency across multi-row transactions.

> [!WARNING]
> **Out of scope:**
> * Specific syntax for vendor-specific dialects (e.g., T-SQL, PL/SQL, PL/pgSQL).
> * General database performance tuning outside of trigger execution.
> * Application-level middleware logic.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Transition Table/Set | A conceptual or virtual table containing the rows affected by the DML statement, representing the "Before" or "After" state. |
| Set-Based Processing | A paradigm where operations are applied to a collection of data simultaneously, rather than iterating through items one by one. |
| Row-Level Trigger | A trigger that executes once for every individual row affected by a DML statement. |
| Statement-Level Trigger | A trigger that executes once per DML statement, regardless of how many rows are affected. |
| RBAR | "Row By Agonizing Row"; a pejorative term for procedural iteration where set-based logic should have been applied. |
| Vectorized Logic | Logic designed to handle arrays or sets of data in a single execution pass. |

## Core Concepts
The fundamental challenge of handling multiple rows in triggers is the shift from **scalar thinking** to **vector thinking**. 

When a trigger is invoked, the database engine provides access to the data being changed. In a multi-row scenario, this data is not a single record but a "Transition Set." 

1. **Transition States**: Triggers typically have access to two states:
    * **The Old State**: The data as it existed before the operation (relevant for `UPDATE` and `DELETE`).
    * **The New State**: The data as it will exist (or does exist) after the operation (relevant for `INSERT` and `UPDATE`).

2. **The Multi-Row Fallacy**: A common error is designing a trigger that expects only one row in the transition set. If a system is designed this way, a bulk update will either fail, process only the first row, or apply the logic of the first row to all subsequent rows incorrectly.

> [!TIP]
> Think of a trigger not as a "callback for a record," but as a "post-processor for a temporary table." If you treat the incoming data as a table that needs to be joined with your existing schema, the logic will naturally scale from one row to one million rows.

## Standard Model
The standard model for handling multiple rows involves the following steps:

1. **Capture the Set**: Access the transition tables (often referred to as `Inserted`/`Deleted` or `New`/`Old` sets).
2. **Join with Target**: Perform a relational `JOIN` between the transition set and the physical tables in the database to identify affected dependencies.
3. **Apply Vectorized Logic**: Use aggregate functions (`SUM`, `COUNT`, `AVG`) or set-based updates to modify related data.
4. **Validate the Batch**: Ensure that constraints or business rules are met for the entire set. If one row in the batch violates a rule, the standard model typically dictates that the entire batch (and the triggering statement) should be rolled back to maintain atomicity.

## Common Patterns
* **The Join Pattern**: Instead of selecting variables from a single row, the trigger performs an `UPDATE` or `INSERT` statement that joins the transition table with a target table.
* **The Aggregation Pattern**: When a trigger is used to maintain summary tables (e.g., updating a `TotalSales` column in a `Categories` table), it calculates the sum of all rows in the transition set and applies the delta to the target in one operation.
* **The Existence Check**: Using `EXISTS` or `IN` clauses against the transition set to determine if any row in the batch meets a specific condition requiring side effects.

## Anti-Patterns
* **Procedural Looping (Cursors)**: Explicitly looping through the transition set to perform actions row-by-row. This is computationally expensive and negates the performance benefits of the database engine's optimizer.
* **Scalar Variable Assignment**: Assigning a value from a transition table to a single scalar variable. In multi-row operations, this usually results in the variable holding the value of the "last" row processed, losing all other data in the batch.
* **Singleton Selects**: Performing a `SELECT...WHERE ID = {SingleID}` inside a trigger without accounting for the fact that multiple IDs may be present in the calling context.

> [!CAUTION]
> Avoid circular dependencies or tight coupling where a multi-row trigger on Table A updates Table B, which in turn triggers an update back on Table A. In multi-row contexts, this can lead to exponential performance degradation or stack overflow errors.

## Edge Cases
* **Zero-Row Affected**: A DML statement may execute successfully but affect zero rows (e.g., `DELETE FROM Users WHERE 1=0`). Triggers must be able to handle an empty transition set without throwing errors.
* **Large Object (LOB) Data**: Handling multiple rows that each contain large binary or text data can lead to memory exhaustion if the trigger attempts to cache the transition set in memory.
* **Recursive Triggers**: If a multi-row update triggers another update on the same table, the system must handle the nested sets correctly. Most systems have a maximum recursion depth.
* **Constraint Timing**: In some systems, triggers fire before constraints are validated. A multi-row trigger might process data that is technically invalid, leading to unexpected rollbacks after the trigger has already performed side effects (like sending an email or logging to an external system).

## Related Topics
* **Atomicity and Rollback**: How multi-row failures affect the transaction state.
* **Set Theory**: The mathematical foundation for relational database operations.
* **Concurrency Control**: How locking mechanisms behave when multiple rows are modified and triggers are fired.
* **Idempotency in Triggers**: Ensuring that re-running logic against the same set of data does not produce unintended side effects.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-14 | Initial AI-generated canonical documentation |